import torch
import torch.nn as nn
import torch.nn.functional as F

class BaseEncoder(nn.Module):
    """
    CNN and multi head attention layer
    Ignores tokens with padding value
    """
    def __init__(self, input_channels=1, embed_dim=512, num_heads=4, negative_value=-9999.0):
        super().__init__()
        self.negative_value = negative_value

        # Convolution layers
        self.conv1 = nn.Conv2d(input_channels, 64, 3, stride=1, padding=1)
        self.bn1   = nn.BatchNorm2d(64)

        self.conv2 = nn.Conv2d(64, 128, 3, stride=1, padding=1)
        self.bn2   = nn.BatchNorm2d(128)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(128, 256, 3, stride=1, padding=1)
        self.bn3   = nn.BatchNorm2d(256)

        self.conv4 = nn.Conv2d(256, embed_dim, 3, stride=1, padding=1)
        self.bn4   = nn.BatchNorm2d(embed_dim)
        self.pool2 = nn.MaxPool2d(2, 2)

        # multi-head attention block
        self.attention = nn.MultiheadAttention(embed_dim=embed_dim,
                                               num_heads=num_heads,
                                               batch_first=True)

        self.pre_attn_norm = nn.LayerNorm(embed_dim)
        self.post_attn_norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x, original_data=None):

        x = F.relu(self.bn1(self.conv1(x)))   
        x = F.relu(self.bn2(self.conv2(x)))   
        x = self.pool1(x)                     
        x = F.relu(self.bn3(self.conv3(x)))    
        x = F.relu(self.bn4(self.conv4(x)))     
        x = self.pool2(x)                        

        B, E, H2, W2 = x.shape
        seq_len = H2 * W2

        #building padding mask
        if original_data is not None:
            od = original_data.squeeze(1)
            od_down = F.interpolate(od.unsqueeze(1), size=(H2, W2), mode='nearest').squeeze(1)
            od_flat = od_down.view(B, -1)
            key_padding_mask = (od_flat == self.negative_value)

            if key_padding_mask is not None:
                mask_percent = key_padding_mask.float().mean(dim=1)
                threshold = 0.95

                for i in range(key_padding_mask.size(0)):
                    if mask_percent[i] >= threshold:
                        key_padding_mask[i, 0] = False

        else:
            key_padding_mask = None

        # Attention layers
        x = x.view(B, E, seq_len).transpose(1, 2)
        x_norm = self.pre_attn_norm(x)
        attn_out, attn_weights = self.attention(x_norm, x_norm, x_norm, key_padding_mask=key_padding_mask)
        x = x + self.dropout(attn_out)
        x = self.post_attn_norm(x)
        x = x.mean(dim=1)  

        return x

class ProjectionHead(nn.Module):
    """
    Projection head as per SimCLR paper: a 2-layer MLP with ReLU activation.
    """
    def __init__(self, input_dim, hidden_dim=2048, output_dim=128):
        super(ProjectionHead, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x


class SimCLR(nn.Module):
    """
    SimCLR model that includes an encoder and a projection head.
    """
    def __init__(self, base_encoder, projection_dim=128, temperature=0.5, device='cuda'):

        super(SimCLR, self).__init__()
        self.encoder = base_encoder.to(device)
        self.device = device

        with torch.no_grad():
            dummy_input = torch.randn(1, 1, 30, 75).to(device)
            encoder_output = self.encoder(dummy_input, original_data=dummy_input)
            encoder_output_dim = encoder_output.shape[-1]
            if len(encoder_output.shape) > 1:
                encoder_output_dim = encoder_output.shape[1]

        self.projection_head = ProjectionHead(input_dim=encoder_output_dim, output_dim=projection_dim)

        self.temperature = temperature

    def forward(self, x, original_data):
        """
        Forward pass through encoder and projection head.

        Args:
            x (torch.Tensor): Input batch.

        Returns:
            torch.Tensor: Projected features.
        """
        h = self.encoder(x, original_data=original_data)
        z = self.projection_head(h)
        z = F.normalize(z, dim=1)
        return z

    def compute_loss(self, z_i, z_j):
        """
        Compute the NT-Xent loss.

        Args:
            z_i (torch.Tensor): Projected features from the first set of augmentations.
            z_j (torch.Tensor): Projected features from the second set of augmentations.

        Returns:
            torch.Tensor: Loss value.
        """
        return self.nt_xent_loss(z_i, z_j)

    def nt_xent_loss(self, z_i, z_j):
        """
        NT-Xent loss function as used in SimCLR.
        Args:
            z_i (torch.Tensor): Projected features from the first set of augmentations.
            z_j (torch.Tensor): Projected features from the second set of augmentations.

        Returns:
            loss: loss value.
        """
        batch_size = z_i.size(0)
        z = torch.cat([z_i, z_j], dim=0)

        # computes similarity matrix
        xcs = F.cosine_similarity(z[:, None, :], z[None, :, :], dim=-1)

        # fill diagonal with -inf for same elements
        xcs[torch.eye(xcs.size(0)).bool()] = float("-inf")

        # Positive pair indices
        target = torch.arange(batch_size, device=xcs.device)
        target = torch.cat([target + batch_size, target])

        xcs /= self.temperature

        loss = F.cross_entropy(xcs, target, reduction="mean")
        return loss


def SimCLR_topk_accuracy(z_i, z_j, temperature=0.5, top_k=1):
    """
    Computes Top-k accuracy for self-supervised embeddings.

    Args:
        z_i (torch.Tensor): Projected features from the first set of augmentations. Shape: [batch_size, dim]
        z_j (torch.Tensor): Projected features from the second set of augmentations. Shape: [batch_size, dim]
        temperature (float): Temperature parameter.
        top_k (int): The 'k' in Top-k accuracy.

    Returns:
        float: Top-k accuracy as a percentage.
    """
    batch_size = z_i.size(0)
    z = torch.cat([z_i, z_j], dim=0)
    z = F.normalize(z, dim=1)

    # compute similarity matrix
    sim = torch.matmul(z, z.T) / temperature

    # mask self-similarity from matrix
    mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)
    sim.masked_fill_(mask, float('-inf'))

    # get the indices of the top k most similar embeddings
    _, indices = sim.topk(k=top_k, dim=1)

    labels = torch.arange(batch_size).to(z.device)
    labels = torch.cat([labels + batch_size, labels], dim=0)  # positive pair indices

    # expand labels to match the size of indices
    labels = labels.unsqueeze(1).expand(-1, top_k)

    # Compare indices with labels
    correct = (indices == labels).any(dim=1).float()

    # score
    accuracy = correct.mean().item() * 100

    return accuracy


